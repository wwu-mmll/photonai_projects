{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PHOTONAI Projects Documentation","text":"<p>Welcome to the PHOTONAI Projects toolbox \u2014 a lightweight framework for organizing, running, and statistically comparing PHOTONAI analyses.</p> <p>This package helps you:</p> <ul> <li>create and manage multiple analyses inside a structured project folder  </li> <li>run PHOTONAI Hyperpipes on stored datasets  </li> <li>perform permutation tests (locally or on SLURM clusters)   </li> <li>compare multiple analyses statistically (using Nadeau-Bengio or permutations)</li> </ul>"},{"location":"#project-management","title":"Project Management","text":""},{"location":"#project-structure","title":"Project Structure","text":"<p>Each PHOTONAI analysis is stored in a clean folder structure:</p> <pre><code>project/\n\u251c\u2500\u2500 analysis_1/\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 X.npy\n\u2502   \u2502   \u2514\u2500\u2500 y.npy\n\u2502   \u251c\u2500\u2500 permutations/\n\u2502   \u2502   \u251c\u2500\u2500 0/\n\u2502   \u2502   \u251c\u2500\u2500 1/\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 slurm_job.cmd\n\u2502   \u251c\u2500\u2500 hyperpipe_constructor.py\n\u2502   \u2514\u2500\u2500 hyperpipe_meta.json\n\u251c\u2500\u2500 analysis_2/\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"#create-project","title":"Create Project","text":"<p>At the beginning of your project, create a new PHOTONAI project.</p> <pre><code>project = PhotonaiProject(project_folder='project')\nproject.add(name=\"analysis_name\", \n            X=X, \n            y=y, \n            hyperpipe_script=\"path/to/hyperpipe_constructor.py\",\n            name_hyperpipe_constructor=\"create_hyperpipe\")\n</code></pre>"},{"location":"#add-analysis","title":"Add Analysis","text":"<p>Then add PHOTONAI analyses using .add(). </p> <pre><code>project.add(name=\"analysis_name\", \n            X=X, \n            y=y, \n            hyperpipe_script=\"path/to/hyperpipe_constructor.py\",\n            name_hyperpipe_constructor=\"create_hyperpipe\")\n</code></pre>"},{"location":"#run-analysis","title":"Run Analysis","text":"<p>Simply use the .run() method to run a specific analysis that has already been added:</p> <pre><code>project.run(\"analysis_name\")\n</code></pre>"},{"location":"#statistics","title":"Statistics","text":""},{"location":"#permutation-test","title":"Permutation Test","text":"<p>Generate permutation-based null distributions for a specific analysis:</p> <pre><code>project.run_permutation_test(\"analysis_name\", n_perms=1000)\n</code></pre>"},{"location":"#compare-analyses","title":"Compare Analyses","text":"<p>Compare two analyses using:</p> <ul> <li><code>method=\"nadeau-bengio\"</code></li> <li><code>method=\"permutation\"</code></li> </ul> <pre><code>project.compare_analyses(\"analysis_1\", \"analysis_2\", method=\"permutation\")\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>Manage and compare multiple PHOTONAI analyses within a single project folder.</p> <p>This class helps you: - create and register new analyses, - run PHOTONAI hyperpipes on stored data, - run permutation tests (locally or on SLURM), - aggregate permutation results, - compute permutation-based p-values, and - statistically compare multiple analyses (Nadeau\u2013Bengio and permutation-based).</p> Source code in <code>photonai_projects/project.py</code> <pre><code>class PhotonaiProject:\n    \"\"\"\n    Manage and compare multiple PHOTONAI analyses within a single project folder.\n\n    This class helps you:\n    - create and register new analyses,\n    - run PHOTONAI hyperpipes on stored data,\n    - run permutation tests (locally or on SLURM),\n    - aggregate permutation results,\n    - compute permutation-based p-values, and\n    - statistically compare multiple analyses (Nadeau\u2013Bengio and permutation-based).\n    \"\"\"\n\n    def __init__(\n        self,\n        project_folder: str,\n        feature_importances: bool = False,\n    ):\n        \"\"\"\n        Initialize a PHOTONAI project.\n\n        Parameters\n        ----------\n        project_folder : str\n            Path to the root folder of the project. All analyses and results are\n            stored inside this folder.\n        feature_importances : bool, optional\n            Whether to compute feature importances (not yet used in this class),\n            by default False.\n        \"\"\"\n        self.project_folder = project_folder\n        self.feature_importances = feature_importances\n        self.reporter = Reporter(self.project_folder)\n        os.makedirs(self.project_folder, exist_ok=True)\n\n    def run(self, name: str):\n        \"\"\"\n        Run a PHOTONAI analysis that has already been added to the project.\n\n        This will:\n        - load the hyperpipe constructor from the analysis folder,\n        - load the stored data `X.npy` and `y.npy`,\n        - fit the hyperpipe, and\n        - write PHOTONAI results to the analysis folder.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis (subfolder of `project_folder`).\n\n        Returns\n        -------\n        Hyperpipe\n            The fitted PHOTONAI hyperpipe instance.\n\n        Raises\n        ------\n        ValueError\n            If the analysis folder does not exist in the project folder.\n        \"\"\"\n        # check that analysis folder exists\n        if name not in os.listdir(self.project_folder):\n            raise ValueError(\n                f\"Analysis {name} not found in project folder {self.project_folder}\"\n            )\n\n        analysis_folder = os.path.join(self.project_folder, name)\n        data_folder = os.path.join(analysis_folder, \"data\")\n\n        pipe = self._load_hyperpipe(analysis_folder, name)\n        pipe.output_settings.set_project_folder(analysis_folder)\n        pipe.output_settings.set_log_file()\n        pipe.name = name\n        pipe.project_folder = analysis_folder\n\n        # load data\n        X = np.load(os.path.join(data_folder, \"X.npy\"))\n        y = np.load(os.path.join(data_folder, \"y.npy\"))\n\n        pipe.fit(X, y)\n\n        # if you want to use feature_importances later, you can hook it here\n        # if self.feature_importances:\n        #     ...\n\n        return pipe\n\n    @staticmethod\n    def _load_hyperpipe(analysis_folder: str, name: str, perm_run: bool = False):\n        \"\"\"\n        Load and instantiate the hyperpipe constructor for a given analysis.\n\n        The analysis folder must contain:\n        - ``hyperpipe_meta.json`` with the key ``\"name_hyperpipe_constructor\"``.\n        - ``hyperpipe_constructor.py`` defining that constructor.\n\n        Parameters\n        ----------\n        analysis_folder : str\n            Path to the analysis folder.\n        name : str\n            Name of the analysis (used to uniquely name the imported module).\n        perm_run : bool, optional\n            If True, reduce verbosity of the pipeline (for permutation runs),\n            by default False.\n\n        Returns\n        -------\n        Hyperpipe\n            Instantiated PHOTONAI hyperpipe.\n\n        Raises\n        ------\n        FileNotFoundError\n            If required metadata or constructor files are missing.\n        KeyError\n            If the constructor name is not found in the metadata file.\n        AttributeError\n            If the constructor function is not found in the constructor module.\n        \"\"\"\n        # ------------------------------------------------------------------\n        # LOAD HYPERPIPE CONSTRUCTOR FROM HYPERPIPE SCRIPT\n        # ------------------------------------------------------------------\n\n        # 1) read metadata to get the constructor function name\n        meta_path = os.path.join(analysis_folder, \"hyperpipe_meta.json\")\n        if not os.path.isfile(meta_path):\n            raise FileNotFoundError(\n                f\"No 'hyperpipe_meta.json' found for analysis '{name}' at {meta_path}. \"\n                f\"Did you create this analysis with 'add'?\"\n            )\n\n        with open(meta_path, \"r\") as f:\n            meta = json.load(f)\n\n        constructor_name = meta.get(\"name_hyperpipe_constructor\", None)\n        if constructor_name is None:\n            raise KeyError(f\"'name_hyperpipe_constructor' not found in {meta_path}\")\n\n        # 2) load the hyperpipe_constructor.py as a module\n        module_path = os.path.join(analysis_folder, \"hyperpipe_constructor.py\")\n        if not os.path.isfile(module_path):\n            raise FileNotFoundError(\n                f\"No 'hyperpipe_constructor.py' found for analysis '{name}' at {module_path}\"\n            )\n\n        spec = importlib.util.spec_from_file_location(\n            f\"hyperpipe_constructor_{name}\", module_path\n        )\n        module = importlib.util.module_from_spec(spec)\n        sys.modules[spec.name] = module\n        spec.loader.exec_module(module)\n\n        if not hasattr(module, constructor_name):\n            raise AttributeError(f\"Function '{constructor_name}' not found in {module_path}\")\n\n        hyperpipe_constructor = getattr(module, constructor_name)\n\n        # 3) build and run the Hyperpipe\n        pipe = hyperpipe_constructor()  # adapt if your constructor needs arguments\n        if perm_run:\n            pipe.verbosity = -1\n        return pipe\n\n    def add(\n        self,\n        name: str,\n        X: np.ndarray,\n        y: np.ndarray,\n        hyperpipe_script: str,\n        name_hyperpipe_constructor: str,\n        **kwargs,\n    ):\n        \"\"\"\n        Register a new analysis in the project.\n\n        This will:\n        - create an analysis subfolder in ``project_folder``,\n        - save `X` and `y` as NumPy arrays,\n        - copy the hyperpipe script into the analysis folder, and\n        - write ``hyperpipe_meta.json`` with the constructor function name.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis (subfolder name).\n        X : np.ndarray\n            Feature matrix with shape (n_samples, n_features).\n        y : np.ndarray\n            Target vector with shape (n_samples,).\n        hyperpipe_script : str\n            Path to the Python script that defines the hyperpipe constructor.\n        name_hyperpipe_constructor : str\n            Name of the hyperpipe constructor function inside `hyperpipe_script`.\n        **kwargs :\n            Additional keyword arguments (currently unused, reserved for future use).\n\n        Raises\n        ------\n        ValueError\n            If `hyperpipe_script` or `name_hyperpipe_constructor` are not provided.\n        \"\"\"\n        if hyperpipe_script is None:\n            raise ValueError(\"hyperpipe_script must be provided in add.\")\n        if name_hyperpipe_constructor is None:\n            raise ValueError(\"name_hyperpipe_constructor must be provided in add.\")\n\n        # create directories for analysis and data\n        analysis_folder = os.path.join(self.project_folder, name)\n        os.makedirs(analysis_folder, exist_ok=True)\n        os.makedirs(os.path.join(analysis_folder, \"data\"), exist_ok=True)\n\n        # save data to numpy array\n        np.save(os.path.join(analysis_folder, \"data\", \"X.npy\"), X)\n        np.save(os.path.join(analysis_folder, \"data\", \"y.npy\"), y)\n\n        # copy script that contains the hyperpipe definition\n        shutil.copyfile(\n            hyperpipe_script,\n            os.path.join(analysis_folder, \"hyperpipe_constructor.py\"),\n        )\n\n        # save metadata (constructor function name etc.)\n        meta = {\n            \"name_hyperpipe_constructor\": name_hyperpipe_constructor\n            # you could add more fields here (e.g. timestamp, description, etc.)\n        }\n        meta_path = os.path.join(analysis_folder, \"hyperpipe_meta.json\")\n        with open(meta_path, \"w\") as f:\n            json.dump(meta, f, indent=2)\n\n    def list_analyses(self) -&gt; None:\n        \"\"\"\n        Print a list of all analyses available in the project folder.\n\n        The function scans the project folder for subdirectories and prints them\n        as available analyses.\n        \"\"\"\n        analyses = [\n            item\n            for item in os.listdir(self.project_folder)\n            if os.path.isdir(os.path.join(self.project_folder, item))\n        ]\n        print(\"Available PHOTONAI analyses are:\")\n        for analysis in analyses:\n            print(f\"  - {analysis}\")\n\n    def run_permutation_test(\n        self,\n        name: str,\n        n_perms: int = 1000,\n        random_state: int = 15,\n        overwrite: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Run a local permutation test for a given analysis.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis.\n        n_perms : int, optional\n            Total number of permutation runs, by default 1000.\n        random_state : int, optional\n            Base random state for generating permutations, by default 15.\n        overwrite : bool, optional\n            If True, overwrite existing permutation results. If False,\n            skip permutations that already have results, by default False.\n        \"\"\"\n        perm_runs = range(n_perms)\n        self._run_permutation_test(\n            name=name,\n            random_state=random_state,\n            n_perms=n_perms,\n            overwrite=overwrite,\n            perm_runs=perm_runs,\n        )\n\n    def check_permutation_test(\n        self,\n        name: str,\n        n_perms: int = 1000,\n    ):\n        \"\"\"\n        Check which permutation runs have a stored PHOTONAI results file.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis.\n        n_perms : int, optional\n            Expected number of permutation runs, by default 1000.\n\n        Returns\n        -------\n        list of int\n            Sorted list of permutation run indices that were found.\n        list of int\n            Sorted list of permutation run indices that are missing.\n        \"\"\"\n        perm_runs = range(n_perms)\n        perm_folder = Path(self.project_folder) / name / \"permutations\"\n\n        found_runs = [\n            int(folder.name)\n            for folder in perm_folder.iterdir()\n            if folder.is_dir() and (folder / \"photonai_results.json\").exists()\n        ]\n        missing_runs = sorted(set(perm_runs) - set(found_runs))\n        print(\n            f\"Found {len(found_runs)} permutation runs, {len(missing_runs)} are missing.\"\n        )\n        return sorted(found_runs), missing_runs\n\n    def _load_true_fold_results(self, name: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Load per-outer-fold performance metrics for an analysis.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis.\n\n        Returns\n        -------\n        pandas.DataFrame\n            DataFrame where rows correspond to outer folds and columns to metrics.\n\n        Raises\n        ------\n        FileNotFoundError\n            If no PHOTONAI run can be found for the given analysis.\n        \"\"\"\n        photonai_folder = find_latest_photonai_run(Path(self.project_folder) / name)\n        if photonai_folder is None:\n            raise FileNotFoundError(\n                f\"No PHOTONAI run found for analysis {name} in {self.project_folder}\"\n            )\n\n        handler = ResultsHandler()\n        handler.load_from_file(str(Path(photonai_folder) / \"photonai_results.json\"))\n        return pd.DataFrame(handler.get_performance_outer_folds())\n\n    def _load_true_results(self, name: str) -&gt; pd.Series:\n        \"\"\"\n        Load mean performance metrics across outer folds for an analysis.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis.\n\n        Returns\n        -------\n        pandas.Series\n            Series of mean metric values indexed by metric name.\n        \"\"\"\n        folds_df = self._load_true_fold_results(name)\n        return folds_df.mean(axis=0)\n\n    def _ensure_and_load_permutation_results(\n        self,\n        name: str,\n        n_perms: int = 1000,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Ensure that aggregated permutation results exist and load them.\n\n        If ``permutation_results.csv`` is missing, it is created by calling\n        :meth:`aggregate_permutation_test`.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis.\n        n_perms : int, optional\n            Number of permutations expected, by default 1000.\n\n        Returns\n        -------\n        pandas.DataFrame\n            DataFrame containing aggregated permutation results with a ``run`` column.\n        \"\"\"\n        perm_results_file = Path(self.project_folder) / name / \"permutation_results.csv\"\n        if not perm_results_file.exists():\n            self.aggregate_permutation_test(name, n_perms)\n        return pd.read_csv(perm_results_file)\n\n    # -------------------------------------------------\n    # Permutation aggregation / p-values\n    # -------------------------------------------------\n    def aggregate_permutation_test(self, name: str, n_perms: int = 1000) -&gt; None:\n        \"\"\"\n        Aggregate results from individual permutation runs into a single CSV file.\n\n        This function:\n        - collects mean outer-fold metrics for each permutation run,\n        - ensures that all permutation indices `0..n_perms-1` are represented,\n        - fills missing values with \u00b1\u221e depending on whether higher is better, and\n        - writes the result to ``permutation_results.csv`` in the analysis folder.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis.\n        n_perms : int, optional\n            Number of permutation runs, by default 1000.\n        \"\"\"\n        perm_folder = Path(self.project_folder) / name / \"permutations\"\n        valid_runs, missing_runs = self.check_permutation_test(name, n_perms)\n\n        outer_folds_metrics = []\n        for valid_run in valid_runs:\n            print(f\"Aggregating results for permutation run {valid_run + 1}/{n_perms}\")\n            handler = ResultsHandler()\n            handler.load_from_file(\n                str(perm_folder / str(valid_run) / \"photonai_results.json\")\n            )\n            mean_metrics = pd.DataFrame(\n                handler.get_performance_outer_folds()\n            ).mean(axis=0)\n            mean_metrics[\"run\"] = valid_run\n            outer_folds_metrics.append(mean_metrics)\n\n        perm_results = pd.DataFrame(outer_folds_metrics)\n\n        # Ensure all runs 0..n_perms-1 are represented\n        df_perm_index = pd.DataFrame(\n            np.arange(n_perms), columns=[\"run\"], index=np.arange(n_perms)\n        )\n        perm_results = pd.merge(df_perm_index, perm_results, on=\"run\", how=\"left\")\n\n        for metric in list(perm_results.keys()):\n            if metric == \"run\":\n                continue\n            greater_is_better = Scorer.greater_is_better_distinction(metric)\n            if greater_is_better:\n                perm_results[metric] = perm_results[metric].fillna(np.inf)\n            else:\n                perm_results[metric] = perm_results[metric].fillna(-np.inf)\n\n        perm_results.to_csv(\n            Path(self.project_folder) / name / \"permutation_results.csv\", index=False\n        )\n\n    def calculate_permutation_p_values(\n        self,\n        name: str,\n        n_perms: int = 1000,\n    ) -&gt; None:\n        \"\"\"\n        Compute permutation-based p-values for a given analysis.\n\n        For each metric, this function compares the true mean performance to the\n        distribution of permutation results and computes a one-sided p-value\n        using the standard (k+1)/(n_perms+1) formulation.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis.\n        n_perms : int, optional\n            Number of permutation runs, by default 1000.\n        \"\"\"\n        true_results = self._load_true_results(name)\n        perm_results = self._ensure_and_load_permutation_results(name, n_perms)\n\n        p_values: Dict[str, float] = {}\n        for metric in list(true_results.keys()):\n            greater_is_better = Scorer.greater_is_better_distinction(metric)\n            current_perm_results = np.asarray(perm_results[metric], dtype=float)\n\n            if greater_is_better:\n                current_perm_results[np.isnan(current_perm_results)] = np.inf\n                p_values[metric] = (\n                    np.sum(true_results[metric] &lt; current_perm_results) + 1\n                ) / (n_perms + 1)\n            else:\n                current_perm_results[np.isnan(current_perm_results)] = -np.inf\n                p_values[metric] = (\n                    np.sum(true_results[metric] &gt; current_perm_results) + 1\n                ) / (n_perms + 1)\n\n            n_valid = n_perms - np.sum(np.isinf(current_perm_results))\n            print(\n                f\"p-value for {metric}: {p_values[metric]} \"\n                f\"(based on n={n_valid} valid permutations)\"\n            )\n\n        pd.DataFrame(p_values, index=[0]).to_csv(\n            Path(self.project_folder) / name / \"permutation_p_values.csv\",\n            index=False,\n        )\n\n    # -------------------------------------------------\n    # Nadeau\u2013Bengio helper\n    # -------------------------------------------------\n    @staticmethod\n    def _nadeau_bengio_p_value(\n        diffs: np.ndarray,\n        n_train: int,\n        n_test: int,\n    ) -&gt; Tuple[float, float]:\n        \"\"\"\n        Two-sided Nadeau &amp; Bengio corrected resampled t-test.\n\n        Parameters\n        ----------\n        diffs : np.ndarray\n            Array of per-fold score differences (analysis2 - analysis1).\n        n_train : int\n            Number of training samples used in each resample.\n        n_test : int\n            Number of test samples used in each resample.\n\n        Returns\n        -------\n        float\n            Two-sided p-value of the test.\n        float\n            t-statistic of the corrected t-test.\n\n        Notes\n        -----\n        The corrected variance is computed as:\n\n        .. math::\n\n            \\\\text{Var}_c = \\\\left(\\\\frac{1}{k} + \\\\frac{n_{test}}{n_{train}}\\\\right) s^2\n        \"\"\"\n        diffs = np.asarray(diffs, dtype=float)\n        k = len(diffs)\n        if k &lt; 2:\n            return 1.0, 0.0  # not enough folds\n\n        mean_diff = np.mean(diffs)\n        var_diff = np.var(diffs, ddof=1)\n        rho = n_test / n_train\n        corrected_var = (1.0 / k + rho) * var_diff\n        if corrected_var &lt;= 0:\n            return 1.0, 0.0\n\n        t_stat = mean_diff / np.sqrt(corrected_var)\n        df = k - 1\n\n        # two-sided p-value\n        p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n        return p_value, t_stat\n\n    # -------------------------------------------------\n    # Comparison of two analyses\n    # -------------------------------------------------\n    def compare_analyses(\n        self,\n        first_analysis: str,\n        second_analysis: str,\n        method: Literal[\"nadeau-bengio\", \"permutation\"] = \"nadeau-bengio\",\n        metric: str | None = None,\n        n_perms: int = 1000,\n        n_train: int | None = None,\n        n_test: int | None = None,\n        print_report: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Compare two analyses using statistical tests.\n\n        You can choose between:\n        - Nadeau\u2013Bengio corrected t-test on outer-fold scores, or\n        - permutation-based null distribution of performance differences.\n\n        Parameters\n        ----------\n        first_analysis : str\n            Name of the first analysis.\n        second_analysis : str\n            Name of the second analysis.\n        method : {\"nadeau-bengio\", \"permutation\"}, optional\n            Statistical comparison method, by default \"nadeau-bengio\".\n        metric : str or None, optional\n            If given, only compare this metric. If None, compare all metrics\n            common to both analyses, by default None.\n        n_perms : int, optional\n            Number of permutation runs (only for permutation-based comparison),\n            by default 1000.\n        n_train : int or None, optional\n            Number of training samples used during cross-validation (required\n            for Nadeau\u2013Bengio), by default None.\n        n_test : int or None, optional\n            Number of test samples used during cross-validation (required\n            for Nadeau\u2013Bengio), by default None.\n        print_report : bool, optional\n            If True, print a formatted comparison report, by default True.\n\n        Returns\n        -------\n        pandas.DataFrame\n            DataFrame indexed by metric, containing columns such as:\n            ``p_value``, ``effect``, and method-specific fields (e.g. ``t_stat``,\n            ``n_folds`` or ``n_valid_perms``).\n\n        Raises\n        ------\n        ValueError\n            If an invalid method is passed or required parameters are missing.\n        \"\"\"\n        valid_methods = {\"nadeau-bengio\", \"permutation\"}\n        if method not in valid_methods:\n            raise ValueError(\n                f\"Invalid method '{method}'. Valid options are: {valid_methods}\"\n            )\n\n        results: list[dict] = []\n\n        # ---------------- permutation-based comparison ----------------\n        if method == \"permutation\":\n            # Load true and permutation results for both analyses\n            true1 = self._load_true_results(first_analysis)\n            perm1 = self._ensure_and_load_permutation_results(\n                first_analysis, n_perms\n            )\n\n            true2 = self._load_true_results(second_analysis)\n            perm2 = self._ensure_and_load_permutation_results(\n                second_analysis, n_perms\n            )\n\n            # sanity check: runs aligned\n            if not np.array_equal(perm1[\"run\"].values, perm2[\"run\"].values):\n                raise ValueError(\n                    \"Permutation indices (run column) do not match between analyses.\"\n                )\n\n            if metric is None:\n                metrics = set(true1.index).intersection(true2.index)\n            else:\n                metrics = [metric]\n            for metric in metrics:\n                greater_is_better = Scorer.greater_is_better_distinction(metric)\n\n                # true difference: analysis2 - analysis1\n                true_diff = float(true2[metric] - true1[metric])\n\n                # permutation differences per run\n                perm_diff = (\n                    np.asarray(perm2[metric], dtype=float)\n                    - np.asarray(perm1[metric], dtype=float)\n                )\n\n                if greater_is_better:\n                    perm_diff[np.isnan(perm_diff)] = np.inf\n                    p_val = (np.sum(true_diff &lt; perm_diff) + 1) / (n_perms + 1)\n                else:\n                    perm_diff[np.isnan(perm_diff)] = -np.inf\n                    p_val = (np.sum(true_diff &gt; perm_diff) + 1) / (n_perms + 1)\n\n                n_valid = n_perms - np.sum(np.isinf(perm_diff))\n                print(\n                    f\"[permutation] {metric}: p={p_val}, \"\n                    f\"true_diff={true_diff} (n_valid={n_valid})\"\n                )\n\n                results.append(\n                    {\n                        \"metric\": metric,\n                        \"method\": \"permutation\",\n                        \"p_value\": p_val,\n                        \"effect\": true_diff,  # analysis2 - analysis1\n                        \"n_valid_perms\": int(n_valid),\n                    }\n                )\n\n        # ---------------- Nadeau\u2013Bengio comparison ----------------\n        elif method == \"nadeau-bengio\":\n            if n_train is None or n_test is None:\n                raise ValueError(\n                    \"n_train and n_test must be provided for the Nadeau-Bengio test.\"\n                )\n\n            folds1 = self._load_true_fold_results(first_analysis)\n            folds2 = self._load_true_fold_results(second_analysis)\n\n            if metric is None:\n                metrics = set(folds1.columns).intersection(folds2.columns)\n            else:\n                metrics = [metric]\n            for metric in metrics:\n                # fold-wise differences: analysis2 - analysis1\n                diffs = folds2[metric].values - folds1[metric].values\n                p_val, t_stat = self._nadeau_bengio_p_value(\n                    diffs,\n                    n_train=n_train,\n                    n_test=n_test,\n                )\n                mean_diff = float(np.mean(diffs))\n\n                print(\n                    f\"[nadeau-bengio] {metric}: p={p_val}, t={t_stat}, \"\n                    f\"A={folds1[metric].mean()}[{folds1[metric].std()}], \"\n                    f\"B={folds2[metric].mean()}[{folds2[metric].std()}], \"\n                    f\"mean_diff={mean_diff}\"\n                )\n\n                results.append(\n                    {\n                        \"metric\": metric,\n                        \"method\": \"nadeau-bengio\",\n                        \"p_value\": p_val,\n                        \"t_stat\": t_stat,\n                        \"effect\": mean_diff,  # analysis2 - analysis1\n                        \"n_folds\": len(diffs),\n                    }\n                )\n\n        df = pd.DataFrame(results).set_index(\"metric\")\n        if print_report:\n            self.print_comparison_report(first_analysis, second_analysis, df)\n        return df\n\n    def print_comparison_report(\n        self,\n        first_analysis: str,\n        second_analysis: str,\n        results_df: pd.DataFrame,\n    ) -&gt; None:\n        \"\"\"\n        Print a formatted summary for the comparison of two analyses.\n\n        This report includes, for each metric:\n        - mean and standard deviation of the true performance for both analyses,\n        - the difference (second - first),\n        - the statistical method, and\n        - method-specific statistics (p-value, t-statistic, etc.).\n\n        Parameters\n        ----------\n        first_analysis : str\n            Name of the first analysis.\n        second_analysis : str\n            Name of the second analysis.\n        results_df : pandas.DataFrame\n            Output DataFrame from :meth:`compare_analyses`.\n        \"\"\"\n        # Load true per-fold results to get mean &amp; std\n        folds1 = self._load_true_fold_results(first_analysis)\n        folds2 = self._load_true_fold_results(second_analysis)\n\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"COMPARISON REPORT: {first_analysis}  vs  {second_analysis}\")\n        print(\"=\" * 80)\n\n        for _, row in results_df.reset_index().iterrows():\n            metric = row[\"metric\"]\n            method = row[\"method\"]\n\n            true1 = folds1[metric]\n            true2 = folds2[metric]\n\n            mean1, std1 = true1.mean(), true1.std(ddof=1)\n            mean2, std2 = true2.mean(), true2.std(ddof=1)\n\n            diff = mean2 - mean1\n\n            print(f\"\\n--- Metric: {metric} ---\")\n            print(f\"{first_analysis}: mean={mean1:.4f}, std={std1:.4f}\")\n            print(f\"{second_analysis}: mean={mean2:.4f}, std={std2:.4f}\")\n            print(f\"Difference (second - first): {diff:.4f}\")\n\n            print(f\"\\nMethod: {method}\")\n\n            if method == \"nadeau-bengio\":\n                print(f\"T-statistic: {row.get('t_stat', float('nan')):.4f}\")\n                print(f\"P-value:     {row['p_value']:.6f}\")\n\n            elif method == \"permutation\":\n                print(f\"P-value:     {row['p_value']:.6f}\")\n                print(f\"Valid perms: {row.get('n_valid_perms', 'N/A')}\")\n\n            print(\"-\" * 80)\n\n        print(\"\\n\")\n\n    def compare_multiple_analyses(\n        self,\n        analyses: Iterable[str],\n        method: Literal[\"nadeau-bengio\", \"permutation\"] = \"nadeau-bengio\",\n        metric: str | None = None,\n        n_perms: int = 1000,\n        n_train: int | None = None,\n        n_test: int | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Compare all pairs of analyses using :meth:`compare_analyses`.\n\n        Parameters\n        ----------\n        analyses : iterable of str\n            Names of analyses (e.g. ``[\"A\", \"B\", \"C\", \"D\"]``).\n        method : {\"nadeau-bengio\", \"permutation\"}, optional\n            Which comparison method to use, by default \"nadeau-bengio\".\n        metric : str or None, optional\n            If given, only compare this metric. If None, compare all metrics\n            common to each pair, by default None.\n        n_perms : int, optional\n            Number of permutations (for permutation-based comparison),\n            by default 1000.\n        n_train : int, optional\n            Number of training samples (for Nadeau\u2013Bengio).\n        n_test : int, optional\n            Number of test samples (for Nadeau\u2013Bengio).\n\n        Returns\n        -------\n        pandas.DataFrame\n            Long-format table with one row per (metric, pair), including\n            p-values, effect sizes, and method-specific statistics.\n\n        Raises\n        ------\n        ValueError\n            If fewer than two analyses are provided.\n        \"\"\"\n        analyses = list(analyses)\n        if len(analyses) &lt; 2:\n            raise ValueError(\"Need at least two analyses to compare.\")\n\n        all_results = []\n\n        for first, second in combinations(analyses, 2):\n            print(f\"Comparing '{first}' vs '{second}' using {method}...\")\n            pair_df = self.compare_analyses(\n                first_analysis=first,\n                second_analysis=second,\n                method=method,\n                metric=metric,\n                n_perms=n_perms,\n                n_train=n_train,\n                n_test=n_test,\n                print_report=False,\n            )\n\n            # Make sure we don't accidentally mutate the original\n            pair_df = pair_df.copy()\n            pair_df[\"first_analysis\"] = first\n            pair_df[\"second_analysis\"] = second\n\n            # move metric from index to column for stacking\n            pair_df = pair_df.reset_index()  # 'metric' becomes a column\n            all_results.append(pair_df)\n\n        if not all_results:\n            return pd.DataFrame()\n\n        result_df = pd.concat(all_results, ignore_index=True)\n\n        return result_df\n\n    def _run_permutation_test(\n        self,\n        name: str,\n        random_state: int = 15,\n        n_perms: int = 1000,\n        overwrite: bool = False,\n        perm_runs: range = range(1000),\n    ) -&gt; None:\n        \"\"\"\n        Internal helper to run a subset of permutation tests for an analysis.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis.\n        random_state : int, optional\n            Base random state for permutation generation, by default 15.\n        n_perms : int, optional\n            Total number of permutation runs, by default 1000.\n        overwrite : bool, optional\n            Whether to overwrite existing permutation results, by default False.\n        perm_runs : range, optional\n            Iterable of permutation indices to run, by default range(1000).\n\n        Raises\n        ------\n        ValueError\n            If the analysis folder does not exist.\n        \"\"\"\n        # check that analysis folder exists\n        if name not in os.listdir(self.project_folder):\n            raise ValueError(\n                f\"Analysis {name} not found in project folder {self.project_folder}\"\n            )\n\n        analysis_folder = os.path.join(self.project_folder, name)\n        data_folder = os.path.join(analysis_folder, \"data\")\n        perm_folder = os.path.join(analysis_folder, \"permutations\")\n\n        # load data\n        X = np.load(os.path.join(data_folder, \"X.npy\"))\n        y = np.load(os.path.join(data_folder, \"y.npy\"))\n\n        for perm_run in perm_runs:\n            current_perm_folder = os.path.join(perm_folder, str(perm_run))\n            if (\n                not overwrite\n                and os.path.exists(\n                    os.path.join(current_perm_folder, \"photonai_results.json\")\n                )\n            ):\n                print(\n                    f\"Skipping permutation {perm_run + 1}/{n_perms} as it already exists.\"\n                )\n                continue\n\n            print(f\"Running permutation {perm_run + 1}/{n_perms}\")\n            np.random.seed(random_state + perm_run)\n            y_perm = np.random.permutation(y)\n            pipe = self._load_hyperpipe(analysis_folder, name, perm_run=True)\n            pipe.output_settings.set_project_folder(\n                os.path.join(perm_folder, str(perm_run))\n            )\n            pipe.output_settings.set_log_file()\n            pipe.name = name\n            pipe.project_folder = os.path.join(perm_folder, str(perm_run))\n            pipe.fit(X, y_perm)\n            shutil.copyfile(\n                os.path.join(\n                    pipe.output_settings.results_folder, \"photonai_results.json\"\n                ),\n                os.path.join(\n                    os.path.join(perm_folder, str(perm_run)),\n                    \"photonai_results.json\",\n                ),\n            )\n            shutil.rmtree(pipe.output_settings.results_folder)\n\n    def run_permutation_test_slurm(\n        self,\n        name: str,\n        n_perms: int = 1000,\n        random_state: int = 15,\n        overwrite: bool = False,\n        slurm_job_id: int | None = None,\n        n_perms_per_job: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Run a subset of permutation tests for use in a SLURM array job.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis.\n        n_perms : int, optional\n            Total number of permutation runs, by default 1000.\n        random_state : int, optional\n            Base random state for permutation generation, by default 15.\n        overwrite : bool, optional\n            Whether to overwrite existing permutation results, by default False.\n        slurm_job_id : int or None, optional\n            Index of the SLURM array job (starting at 1).\n        n_perms_per_job : int or None, optional\n            Number of permutations to run in this job.\n        \"\"\"\n        perms_to_do = np.arange(\n            (slurm_job_id - 1) * n_perms_per_job,\n            (slurm_job_id - 1) * n_perms_per_job + n_perms_per_job,\n        )\n        self._run_permutation_test(\n            name=name,\n            random_state=random_state,\n            n_perms=n_perms,\n            overwrite=overwrite,\n            perm_runs=perms_to_do,\n        )\n\n    def prepare_slurm_permutation_test(\n        self,\n        name: str,\n        n_perms: int,\n        conda_env: str,\n        memory_per_cpu: int,\n        n_jobs: int,\n        run_time: str = \"0-01:00:00\",\n        random_state: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Prepare a SLURM job script for running permutation tests in parallel.\n\n        This function:\n        - computes how many permutations each SLURM array job should run,\n        - copies the current project script into the project folder, and\n        - writes a SLURM script that calls :func:`run_perm_job`.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis.\n        n_perms : int\n            Total number of permutation runs.\n        conda_env : str\n            Name of the conda environment to activate in the SLURM job.\n        memory_per_cpu : int\n            Memory per CPU in GB.\n        n_jobs : int\n            Number of jobs in the SLURM array.\n        run_time : str, optional\n            Maximum wall time for each job (SLURM time format),\n            by default \"0-01:00:00\".\n        random_state : int, optional\n            Base random state, by default 1.\n\n        Raises\n        ------\n        ValueError\n            If the analysis folder does not exist in the project folder.\n        \"\"\"\n        if name not in os.listdir(self.project_folder):\n            raise ValueError(\n                f\"Analysis {name} not found in project folder {self.project_folder}\"\n            )\n\n        analysis_folder = os.path.join(self.project_folder, name)\n        # calculate the number of perms per job\n        n_perms_per_job = int(n_perms / n_jobs)\n\n        # copy script that contains the permutation test\n        shutil.copyfile(\n            os.path.abspath(__file__),\n            os.path.join(self.project_folder, os.path.basename(__file__)),\n        )\n\n        # create slurm script\n        cmd = f\"\"\"#!/bin/bash\n\n#SBATCH --job-name={name + \"_perm_test\"}\n#SBATCH --output=logs/job_%a.log\n\n#SBATCH --partition normal\n#SBATCH --mem-per-cpu={memory_per_cpu}G\n#SBATCH --time={run_time}\n#SBATCH --array=1-{n_jobs}\n\n# add python\nmodule load palma/2021a\nmodule load Miniconda3\n\n# activate conda env\neval \"$(conda shell.bash hook)\"\nconda activate {conda_env}\n\n\npython ../project.py --project-folder ../../{self.project_folder} --analysis-name {name} --n-perms {n_perms} --slurm-job-id $SLURM_ARRAY_TASK_ID --n-perms-per-job {n_perms_per_job} --random-state {random_state}\n\"\"\"\n        with open(os.path.join(analysis_folder, \"slurm_job.cmd\"), \"w\") as text_file:\n            text_file.write(cmd)\n\n        return\n\n    def generate_report(self):\n        self.reporter.collect_results()\n        self.reporter.write_report()\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.__init__","title":"<code>__init__(project_folder, feature_importances=False)</code>","text":"<p>Initialize a PHOTONAI project.</p> <p>Parameters:</p> Name Type Description Default <code>project_folder</code> <code>str</code> <p>Path to the root folder of the project. All analyses and results are stored inside this folder.</p> required <code>feature_importances</code> <code>bool</code> <p>Whether to compute feature importances (not yet used in this class), by default False.</p> <code>False</code> Source code in <code>photonai_projects/project.py</code> <pre><code>def __init__(\n    self,\n    project_folder: str,\n    feature_importances: bool = False,\n):\n    \"\"\"\n    Initialize a PHOTONAI project.\n\n    Parameters\n    ----------\n    project_folder : str\n        Path to the root folder of the project. All analyses and results are\n        stored inside this folder.\n    feature_importances : bool, optional\n        Whether to compute feature importances (not yet used in this class),\n        by default False.\n    \"\"\"\n    self.project_folder = project_folder\n    self.feature_importances = feature_importances\n    self.reporter = Reporter(self.project_folder)\n    os.makedirs(self.project_folder, exist_ok=True)\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.add","title":"<code>add(name, X, y, hyperpipe_script, name_hyperpipe_constructor, **kwargs)</code>","text":"<p>Register a new analysis in the project.</p> <p>This will: - create an analysis subfolder in <code>project_folder</code>, - save <code>X</code> and <code>y</code> as NumPy arrays, - copy the hyperpipe script into the analysis folder, and - write <code>hyperpipe_meta.json</code> with the constructor function name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the analysis (subfolder name).</p> required <code>X</code> <code>ndarray</code> <p>Feature matrix with shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Target vector with shape (n_samples,).</p> required <code>hyperpipe_script</code> <code>str</code> <p>Path to the Python script that defines the hyperpipe constructor.</p> required <code>name_hyperpipe_constructor</code> <code>str</code> <p>Name of the hyperpipe constructor function inside <code>hyperpipe_script</code>.</p> required <code>**kwargs</code> <p>Additional keyword arguments (currently unused, reserved for future use).</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>hyperpipe_script</code> or <code>name_hyperpipe_constructor</code> are not provided.</p> Source code in <code>photonai_projects/project.py</code> <pre><code>def add(\n    self,\n    name: str,\n    X: np.ndarray,\n    y: np.ndarray,\n    hyperpipe_script: str,\n    name_hyperpipe_constructor: str,\n    **kwargs,\n):\n    \"\"\"\n    Register a new analysis in the project.\n\n    This will:\n    - create an analysis subfolder in ``project_folder``,\n    - save `X` and `y` as NumPy arrays,\n    - copy the hyperpipe script into the analysis folder, and\n    - write ``hyperpipe_meta.json`` with the constructor function name.\n\n    Parameters\n    ----------\n    name : str\n        Name of the analysis (subfolder name).\n    X : np.ndarray\n        Feature matrix with shape (n_samples, n_features).\n    y : np.ndarray\n        Target vector with shape (n_samples,).\n    hyperpipe_script : str\n        Path to the Python script that defines the hyperpipe constructor.\n    name_hyperpipe_constructor : str\n        Name of the hyperpipe constructor function inside `hyperpipe_script`.\n    **kwargs :\n        Additional keyword arguments (currently unused, reserved for future use).\n\n    Raises\n    ------\n    ValueError\n        If `hyperpipe_script` or `name_hyperpipe_constructor` are not provided.\n    \"\"\"\n    if hyperpipe_script is None:\n        raise ValueError(\"hyperpipe_script must be provided in add.\")\n    if name_hyperpipe_constructor is None:\n        raise ValueError(\"name_hyperpipe_constructor must be provided in add.\")\n\n    # create directories for analysis and data\n    analysis_folder = os.path.join(self.project_folder, name)\n    os.makedirs(analysis_folder, exist_ok=True)\n    os.makedirs(os.path.join(analysis_folder, \"data\"), exist_ok=True)\n\n    # save data to numpy array\n    np.save(os.path.join(analysis_folder, \"data\", \"X.npy\"), X)\n    np.save(os.path.join(analysis_folder, \"data\", \"y.npy\"), y)\n\n    # copy script that contains the hyperpipe definition\n    shutil.copyfile(\n        hyperpipe_script,\n        os.path.join(analysis_folder, \"hyperpipe_constructor.py\"),\n    )\n\n    # save metadata (constructor function name etc.)\n    meta = {\n        \"name_hyperpipe_constructor\": name_hyperpipe_constructor\n        # you could add more fields here (e.g. timestamp, description, etc.)\n    }\n    meta_path = os.path.join(analysis_folder, \"hyperpipe_meta.json\")\n    with open(meta_path, \"w\") as f:\n        json.dump(meta, f, indent=2)\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.aggregate_permutation_test","title":"<code>aggregate_permutation_test(name, n_perms=1000)</code>","text":"<p>Aggregate results from individual permutation runs into a single CSV file.</p> <p>This function: - collects mean outer-fold metrics for each permutation run, - ensures that all permutation indices <code>0..n_perms-1</code> are represented, - fills missing values with \u00b1\u221e depending on whether higher is better, and - writes the result to <code>permutation_results.csv</code> in the analysis folder.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the analysis.</p> required <code>n_perms</code> <code>int</code> <p>Number of permutation runs, by default 1000.</p> <code>1000</code> Source code in <code>photonai_projects/project.py</code> <pre><code>def aggregate_permutation_test(self, name: str, n_perms: int = 1000) -&gt; None:\n    \"\"\"\n    Aggregate results from individual permutation runs into a single CSV file.\n\n    This function:\n    - collects mean outer-fold metrics for each permutation run,\n    - ensures that all permutation indices `0..n_perms-1` are represented,\n    - fills missing values with \u00b1\u221e depending on whether higher is better, and\n    - writes the result to ``permutation_results.csv`` in the analysis folder.\n\n    Parameters\n    ----------\n    name : str\n        Name of the analysis.\n    n_perms : int, optional\n        Number of permutation runs, by default 1000.\n    \"\"\"\n    perm_folder = Path(self.project_folder) / name / \"permutations\"\n    valid_runs, missing_runs = self.check_permutation_test(name, n_perms)\n\n    outer_folds_metrics = []\n    for valid_run in valid_runs:\n        print(f\"Aggregating results for permutation run {valid_run + 1}/{n_perms}\")\n        handler = ResultsHandler()\n        handler.load_from_file(\n            str(perm_folder / str(valid_run) / \"photonai_results.json\")\n        )\n        mean_metrics = pd.DataFrame(\n            handler.get_performance_outer_folds()\n        ).mean(axis=0)\n        mean_metrics[\"run\"] = valid_run\n        outer_folds_metrics.append(mean_metrics)\n\n    perm_results = pd.DataFrame(outer_folds_metrics)\n\n    # Ensure all runs 0..n_perms-1 are represented\n    df_perm_index = pd.DataFrame(\n        np.arange(n_perms), columns=[\"run\"], index=np.arange(n_perms)\n    )\n    perm_results = pd.merge(df_perm_index, perm_results, on=\"run\", how=\"left\")\n\n    for metric in list(perm_results.keys()):\n        if metric == \"run\":\n            continue\n        greater_is_better = Scorer.greater_is_better_distinction(metric)\n        if greater_is_better:\n            perm_results[metric] = perm_results[metric].fillna(np.inf)\n        else:\n            perm_results[metric] = perm_results[metric].fillna(-np.inf)\n\n    perm_results.to_csv(\n        Path(self.project_folder) / name / \"permutation_results.csv\", index=False\n    )\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.calculate_permutation_p_values","title":"<code>calculate_permutation_p_values(name, n_perms=1000)</code>","text":"<p>Compute permutation-based p-values for a given analysis.</p> <p>For each metric, this function compares the true mean performance to the distribution of permutation results and computes a one-sided p-value using the standard (k+1)/(n_perms+1) formulation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the analysis.</p> required <code>n_perms</code> <code>int</code> <p>Number of permutation runs, by default 1000.</p> <code>1000</code> Source code in <code>photonai_projects/project.py</code> <pre><code>def calculate_permutation_p_values(\n    self,\n    name: str,\n    n_perms: int = 1000,\n) -&gt; None:\n    \"\"\"\n    Compute permutation-based p-values for a given analysis.\n\n    For each metric, this function compares the true mean performance to the\n    distribution of permutation results and computes a one-sided p-value\n    using the standard (k+1)/(n_perms+1) formulation.\n\n    Parameters\n    ----------\n    name : str\n        Name of the analysis.\n    n_perms : int, optional\n        Number of permutation runs, by default 1000.\n    \"\"\"\n    true_results = self._load_true_results(name)\n    perm_results = self._ensure_and_load_permutation_results(name, n_perms)\n\n    p_values: Dict[str, float] = {}\n    for metric in list(true_results.keys()):\n        greater_is_better = Scorer.greater_is_better_distinction(metric)\n        current_perm_results = np.asarray(perm_results[metric], dtype=float)\n\n        if greater_is_better:\n            current_perm_results[np.isnan(current_perm_results)] = np.inf\n            p_values[metric] = (\n                np.sum(true_results[metric] &lt; current_perm_results) + 1\n            ) / (n_perms + 1)\n        else:\n            current_perm_results[np.isnan(current_perm_results)] = -np.inf\n            p_values[metric] = (\n                np.sum(true_results[metric] &gt; current_perm_results) + 1\n            ) / (n_perms + 1)\n\n        n_valid = n_perms - np.sum(np.isinf(current_perm_results))\n        print(\n            f\"p-value for {metric}: {p_values[metric]} \"\n            f\"(based on n={n_valid} valid permutations)\"\n        )\n\n    pd.DataFrame(p_values, index=[0]).to_csv(\n        Path(self.project_folder) / name / \"permutation_p_values.csv\",\n        index=False,\n    )\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.check_permutation_test","title":"<code>check_permutation_test(name, n_perms=1000)</code>","text":"<p>Check which permutation runs have a stored PHOTONAI results file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the analysis.</p> required <code>n_perms</code> <code>int</code> <p>Expected number of permutation runs, by default 1000.</p> <code>1000</code> <p>Returns:</p> Type Description <code>list of int</code> <p>Sorted list of permutation run indices that were found.</p> <code>list of int</code> <p>Sorted list of permutation run indices that are missing.</p> Source code in <code>photonai_projects/project.py</code> <pre><code>def check_permutation_test(\n    self,\n    name: str,\n    n_perms: int = 1000,\n):\n    \"\"\"\n    Check which permutation runs have a stored PHOTONAI results file.\n\n    Parameters\n    ----------\n    name : str\n        Name of the analysis.\n    n_perms : int, optional\n        Expected number of permutation runs, by default 1000.\n\n    Returns\n    -------\n    list of int\n        Sorted list of permutation run indices that were found.\n    list of int\n        Sorted list of permutation run indices that are missing.\n    \"\"\"\n    perm_runs = range(n_perms)\n    perm_folder = Path(self.project_folder) / name / \"permutations\"\n\n    found_runs = [\n        int(folder.name)\n        for folder in perm_folder.iterdir()\n        if folder.is_dir() and (folder / \"photonai_results.json\").exists()\n    ]\n    missing_runs = sorted(set(perm_runs) - set(found_runs))\n    print(\n        f\"Found {len(found_runs)} permutation runs, {len(missing_runs)} are missing.\"\n    )\n    return sorted(found_runs), missing_runs\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.compare_analyses","title":"<code>compare_analyses(first_analysis, second_analysis, method='nadeau-bengio', metric=None, n_perms=1000, n_train=None, n_test=None, print_report=True)</code>","text":"<p>Compare two analyses using statistical tests.</p> <p>You can choose between: - Nadeau\u2013Bengio corrected t-test on outer-fold scores, or - permutation-based null distribution of performance differences.</p> <p>Parameters:</p> Name Type Description Default <code>first_analysis</code> <code>str</code> <p>Name of the first analysis.</p> required <code>second_analysis</code> <code>str</code> <p>Name of the second analysis.</p> required <code>method</code> <code>(nadeau - bengio, permutation)</code> <p>Statistical comparison method, by default \"nadeau-bengio\".</p> <code>\"nadeau-bengio\"</code> <code>metric</code> <code>str or None</code> <p>If given, only compare this metric. If None, compare all metrics common to both analyses, by default None.</p> <code>None</code> <code>n_perms</code> <code>int</code> <p>Number of permutation runs (only for permutation-based comparison), by default 1000.</p> <code>1000</code> <code>n_train</code> <code>int or None</code> <p>Number of training samples used during cross-validation (required for Nadeau\u2013Bengio), by default None.</p> <code>None</code> <code>n_test</code> <code>int or None</code> <p>Number of test samples used during cross-validation (required for Nadeau\u2013Bengio), by default None.</p> <code>None</code> <code>print_report</code> <code>bool</code> <p>If True, print a formatted comparison report, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame indexed by metric, containing columns such as: <code>p_value</code>, <code>effect</code>, and method-specific fields (e.g. <code>t_stat</code>, <code>n_folds</code> or <code>n_valid_perms</code>).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid method is passed or required parameters are missing.</p> Source code in <code>photonai_projects/project.py</code> <pre><code>def compare_analyses(\n    self,\n    first_analysis: str,\n    second_analysis: str,\n    method: Literal[\"nadeau-bengio\", \"permutation\"] = \"nadeau-bengio\",\n    metric: str | None = None,\n    n_perms: int = 1000,\n    n_train: int | None = None,\n    n_test: int | None = None,\n    print_report: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compare two analyses using statistical tests.\n\n    You can choose between:\n    - Nadeau\u2013Bengio corrected t-test on outer-fold scores, or\n    - permutation-based null distribution of performance differences.\n\n    Parameters\n    ----------\n    first_analysis : str\n        Name of the first analysis.\n    second_analysis : str\n        Name of the second analysis.\n    method : {\"nadeau-bengio\", \"permutation\"}, optional\n        Statistical comparison method, by default \"nadeau-bengio\".\n    metric : str or None, optional\n        If given, only compare this metric. If None, compare all metrics\n        common to both analyses, by default None.\n    n_perms : int, optional\n        Number of permutation runs (only for permutation-based comparison),\n        by default 1000.\n    n_train : int or None, optional\n        Number of training samples used during cross-validation (required\n        for Nadeau\u2013Bengio), by default None.\n    n_test : int or None, optional\n        Number of test samples used during cross-validation (required\n        for Nadeau\u2013Bengio), by default None.\n    print_report : bool, optional\n        If True, print a formatted comparison report, by default True.\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame indexed by metric, containing columns such as:\n        ``p_value``, ``effect``, and method-specific fields (e.g. ``t_stat``,\n        ``n_folds`` or ``n_valid_perms``).\n\n    Raises\n    ------\n    ValueError\n        If an invalid method is passed or required parameters are missing.\n    \"\"\"\n    valid_methods = {\"nadeau-bengio\", \"permutation\"}\n    if method not in valid_methods:\n        raise ValueError(\n            f\"Invalid method '{method}'. Valid options are: {valid_methods}\"\n        )\n\n    results: list[dict] = []\n\n    # ---------------- permutation-based comparison ----------------\n    if method == \"permutation\":\n        # Load true and permutation results for both analyses\n        true1 = self._load_true_results(first_analysis)\n        perm1 = self._ensure_and_load_permutation_results(\n            first_analysis, n_perms\n        )\n\n        true2 = self._load_true_results(second_analysis)\n        perm2 = self._ensure_and_load_permutation_results(\n            second_analysis, n_perms\n        )\n\n        # sanity check: runs aligned\n        if not np.array_equal(perm1[\"run\"].values, perm2[\"run\"].values):\n            raise ValueError(\n                \"Permutation indices (run column) do not match between analyses.\"\n            )\n\n        if metric is None:\n            metrics = set(true1.index).intersection(true2.index)\n        else:\n            metrics = [metric]\n        for metric in metrics:\n            greater_is_better = Scorer.greater_is_better_distinction(metric)\n\n            # true difference: analysis2 - analysis1\n            true_diff = float(true2[metric] - true1[metric])\n\n            # permutation differences per run\n            perm_diff = (\n                np.asarray(perm2[metric], dtype=float)\n                - np.asarray(perm1[metric], dtype=float)\n            )\n\n            if greater_is_better:\n                perm_diff[np.isnan(perm_diff)] = np.inf\n                p_val = (np.sum(true_diff &lt; perm_diff) + 1) / (n_perms + 1)\n            else:\n                perm_diff[np.isnan(perm_diff)] = -np.inf\n                p_val = (np.sum(true_diff &gt; perm_diff) + 1) / (n_perms + 1)\n\n            n_valid = n_perms - np.sum(np.isinf(perm_diff))\n            print(\n                f\"[permutation] {metric}: p={p_val}, \"\n                f\"true_diff={true_diff} (n_valid={n_valid})\"\n            )\n\n            results.append(\n                {\n                    \"metric\": metric,\n                    \"method\": \"permutation\",\n                    \"p_value\": p_val,\n                    \"effect\": true_diff,  # analysis2 - analysis1\n                    \"n_valid_perms\": int(n_valid),\n                }\n            )\n\n    # ---------------- Nadeau\u2013Bengio comparison ----------------\n    elif method == \"nadeau-bengio\":\n        if n_train is None or n_test is None:\n            raise ValueError(\n                \"n_train and n_test must be provided for the Nadeau-Bengio test.\"\n            )\n\n        folds1 = self._load_true_fold_results(first_analysis)\n        folds2 = self._load_true_fold_results(second_analysis)\n\n        if metric is None:\n            metrics = set(folds1.columns).intersection(folds2.columns)\n        else:\n            metrics = [metric]\n        for metric in metrics:\n            # fold-wise differences: analysis2 - analysis1\n            diffs = folds2[metric].values - folds1[metric].values\n            p_val, t_stat = self._nadeau_bengio_p_value(\n                diffs,\n                n_train=n_train,\n                n_test=n_test,\n            )\n            mean_diff = float(np.mean(diffs))\n\n            print(\n                f\"[nadeau-bengio] {metric}: p={p_val}, t={t_stat}, \"\n                f\"A={folds1[metric].mean()}[{folds1[metric].std()}], \"\n                f\"B={folds2[metric].mean()}[{folds2[metric].std()}], \"\n                f\"mean_diff={mean_diff}\"\n            )\n\n            results.append(\n                {\n                    \"metric\": metric,\n                    \"method\": \"nadeau-bengio\",\n                    \"p_value\": p_val,\n                    \"t_stat\": t_stat,\n                    \"effect\": mean_diff,  # analysis2 - analysis1\n                    \"n_folds\": len(diffs),\n                }\n            )\n\n    df = pd.DataFrame(results).set_index(\"metric\")\n    if print_report:\n        self.print_comparison_report(first_analysis, second_analysis, df)\n    return df\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.compare_multiple_analyses","title":"<code>compare_multiple_analyses(analyses, method='nadeau-bengio', metric=None, n_perms=1000, n_train=None, n_test=None)</code>","text":"<p>Compare all pairs of analyses using :meth:<code>compare_analyses</code>.</p> <p>Parameters:</p> Name Type Description Default <code>analyses</code> <code>iterable of str</code> <p>Names of analyses (e.g. <code>[\"A\", \"B\", \"C\", \"D\"]</code>).</p> required <code>method</code> <code>(nadeau - bengio, permutation)</code> <p>Which comparison method to use, by default \"nadeau-bengio\".</p> <code>\"nadeau-bengio\"</code> <code>metric</code> <code>str or None</code> <p>If given, only compare this metric. If None, compare all metrics common to each pair, by default None.</p> <code>None</code> <code>n_perms</code> <code>int</code> <p>Number of permutations (for permutation-based comparison), by default 1000.</p> <code>1000</code> <code>n_train</code> <code>int</code> <p>Number of training samples (for Nadeau\u2013Bengio).</p> <code>None</code> <code>n_test</code> <code>int</code> <p>Number of test samples (for Nadeau\u2013Bengio).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Long-format table with one row per (metric, pair), including p-values, effect sizes, and method-specific statistics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If fewer than two analyses are provided.</p> Source code in <code>photonai_projects/project.py</code> <pre><code>def compare_multiple_analyses(\n    self,\n    analyses: Iterable[str],\n    method: Literal[\"nadeau-bengio\", \"permutation\"] = \"nadeau-bengio\",\n    metric: str | None = None,\n    n_perms: int = 1000,\n    n_train: int | None = None,\n    n_test: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compare all pairs of analyses using :meth:`compare_analyses`.\n\n    Parameters\n    ----------\n    analyses : iterable of str\n        Names of analyses (e.g. ``[\"A\", \"B\", \"C\", \"D\"]``).\n    method : {\"nadeau-bengio\", \"permutation\"}, optional\n        Which comparison method to use, by default \"nadeau-bengio\".\n    metric : str or None, optional\n        If given, only compare this metric. If None, compare all metrics\n        common to each pair, by default None.\n    n_perms : int, optional\n        Number of permutations (for permutation-based comparison),\n        by default 1000.\n    n_train : int, optional\n        Number of training samples (for Nadeau\u2013Bengio).\n    n_test : int, optional\n        Number of test samples (for Nadeau\u2013Bengio).\n\n    Returns\n    -------\n    pandas.DataFrame\n        Long-format table with one row per (metric, pair), including\n        p-values, effect sizes, and method-specific statistics.\n\n    Raises\n    ------\n    ValueError\n        If fewer than two analyses are provided.\n    \"\"\"\n    analyses = list(analyses)\n    if len(analyses) &lt; 2:\n        raise ValueError(\"Need at least two analyses to compare.\")\n\n    all_results = []\n\n    for first, second in combinations(analyses, 2):\n        print(f\"Comparing '{first}' vs '{second}' using {method}...\")\n        pair_df = self.compare_analyses(\n            first_analysis=first,\n            second_analysis=second,\n            method=method,\n            metric=metric,\n            n_perms=n_perms,\n            n_train=n_train,\n            n_test=n_test,\n            print_report=False,\n        )\n\n        # Make sure we don't accidentally mutate the original\n        pair_df = pair_df.copy()\n        pair_df[\"first_analysis\"] = first\n        pair_df[\"second_analysis\"] = second\n\n        # move metric from index to column for stacking\n        pair_df = pair_df.reset_index()  # 'metric' becomes a column\n        all_results.append(pair_df)\n\n    if not all_results:\n        return pd.DataFrame()\n\n    result_df = pd.concat(all_results, ignore_index=True)\n\n    return result_df\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.list_analyses","title":"<code>list_analyses()</code>","text":"<p>Print a list of all analyses available in the project folder.</p> <p>The function scans the project folder for subdirectories and prints them as available analyses.</p> Source code in <code>photonai_projects/project.py</code> <pre><code>def list_analyses(self) -&gt; None:\n    \"\"\"\n    Print a list of all analyses available in the project folder.\n\n    The function scans the project folder for subdirectories and prints them\n    as available analyses.\n    \"\"\"\n    analyses = [\n        item\n        for item in os.listdir(self.project_folder)\n        if os.path.isdir(os.path.join(self.project_folder, item))\n    ]\n    print(\"Available PHOTONAI analyses are:\")\n    for analysis in analyses:\n        print(f\"  - {analysis}\")\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.prepare_slurm_permutation_test","title":"<code>prepare_slurm_permutation_test(name, n_perms, conda_env, memory_per_cpu, n_jobs, run_time='0-01:00:00', random_state=1)</code>","text":"<p>Prepare a SLURM job script for running permutation tests in parallel.</p> <p>This function: - computes how many permutations each SLURM array job should run, - copies the current project script into the project folder, and - writes a SLURM script that calls :func:<code>run_perm_job</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the analysis.</p> required <code>n_perms</code> <code>int</code> <p>Total number of permutation runs.</p> required <code>conda_env</code> <code>str</code> <p>Name of the conda environment to activate in the SLURM job.</p> required <code>memory_per_cpu</code> <code>int</code> <p>Memory per CPU in GB.</p> required <code>n_jobs</code> <code>int</code> <p>Number of jobs in the SLURM array.</p> required <code>run_time</code> <code>str</code> <p>Maximum wall time for each job (SLURM time format), by default \"0-01:00:00\".</p> <code>'0-01:00:00'</code> <code>random_state</code> <code>int</code> <p>Base random state, by default 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the analysis folder does not exist in the project folder.</p> Source code in <code>photonai_projects/project.py</code> <pre><code>    def prepare_slurm_permutation_test(\n        self,\n        name: str,\n        n_perms: int,\n        conda_env: str,\n        memory_per_cpu: int,\n        n_jobs: int,\n        run_time: str = \"0-01:00:00\",\n        random_state: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Prepare a SLURM job script for running permutation tests in parallel.\n\n        This function:\n        - computes how many permutations each SLURM array job should run,\n        - copies the current project script into the project folder, and\n        - writes a SLURM script that calls :func:`run_perm_job`.\n\n        Parameters\n        ----------\n        name : str\n            Name of the analysis.\n        n_perms : int\n            Total number of permutation runs.\n        conda_env : str\n            Name of the conda environment to activate in the SLURM job.\n        memory_per_cpu : int\n            Memory per CPU in GB.\n        n_jobs : int\n            Number of jobs in the SLURM array.\n        run_time : str, optional\n            Maximum wall time for each job (SLURM time format),\n            by default \"0-01:00:00\".\n        random_state : int, optional\n            Base random state, by default 1.\n\n        Raises\n        ------\n        ValueError\n            If the analysis folder does not exist in the project folder.\n        \"\"\"\n        if name not in os.listdir(self.project_folder):\n            raise ValueError(\n                f\"Analysis {name} not found in project folder {self.project_folder}\"\n            )\n\n        analysis_folder = os.path.join(self.project_folder, name)\n        # calculate the number of perms per job\n        n_perms_per_job = int(n_perms / n_jobs)\n\n        # copy script that contains the permutation test\n        shutil.copyfile(\n            os.path.abspath(__file__),\n            os.path.join(self.project_folder, os.path.basename(__file__)),\n        )\n\n        # create slurm script\n        cmd = f\"\"\"#!/bin/bash\n\n#SBATCH --job-name={name + \"_perm_test\"}\n#SBATCH --output=logs/job_%a.log\n\n#SBATCH --partition normal\n#SBATCH --mem-per-cpu={memory_per_cpu}G\n#SBATCH --time={run_time}\n#SBATCH --array=1-{n_jobs}\n\n# add python\nmodule load palma/2021a\nmodule load Miniconda3\n\n# activate conda env\neval \"$(conda shell.bash hook)\"\nconda activate {conda_env}\n\n\npython ../project.py --project-folder ../../{self.project_folder} --analysis-name {name} --n-perms {n_perms} --slurm-job-id $SLURM_ARRAY_TASK_ID --n-perms-per-job {n_perms_per_job} --random-state {random_state}\n\"\"\"\n        with open(os.path.join(analysis_folder, \"slurm_job.cmd\"), \"w\") as text_file:\n            text_file.write(cmd)\n\n        return\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.print_comparison_report","title":"<code>print_comparison_report(first_analysis, second_analysis, results_df)</code>","text":"<p>Print a formatted summary for the comparison of two analyses.</p> <p>This report includes, for each metric: - mean and standard deviation of the true performance for both analyses, - the difference (second - first), - the statistical method, and - method-specific statistics (p-value, t-statistic, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>first_analysis</code> <code>str</code> <p>Name of the first analysis.</p> required <code>second_analysis</code> <code>str</code> <p>Name of the second analysis.</p> required <code>results_df</code> <code>DataFrame</code> <p>Output DataFrame from :meth:<code>compare_analyses</code>.</p> required Source code in <code>photonai_projects/project.py</code> <pre><code>def print_comparison_report(\n    self,\n    first_analysis: str,\n    second_analysis: str,\n    results_df: pd.DataFrame,\n) -&gt; None:\n    \"\"\"\n    Print a formatted summary for the comparison of two analyses.\n\n    This report includes, for each metric:\n    - mean and standard deviation of the true performance for both analyses,\n    - the difference (second - first),\n    - the statistical method, and\n    - method-specific statistics (p-value, t-statistic, etc.).\n\n    Parameters\n    ----------\n    first_analysis : str\n        Name of the first analysis.\n    second_analysis : str\n        Name of the second analysis.\n    results_df : pandas.DataFrame\n        Output DataFrame from :meth:`compare_analyses`.\n    \"\"\"\n    # Load true per-fold results to get mean &amp; std\n    folds1 = self._load_true_fold_results(first_analysis)\n    folds2 = self._load_true_fold_results(second_analysis)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"COMPARISON REPORT: {first_analysis}  vs  {second_analysis}\")\n    print(\"=\" * 80)\n\n    for _, row in results_df.reset_index().iterrows():\n        metric = row[\"metric\"]\n        method = row[\"method\"]\n\n        true1 = folds1[metric]\n        true2 = folds2[metric]\n\n        mean1, std1 = true1.mean(), true1.std(ddof=1)\n        mean2, std2 = true2.mean(), true2.std(ddof=1)\n\n        diff = mean2 - mean1\n\n        print(f\"\\n--- Metric: {metric} ---\")\n        print(f\"{first_analysis}: mean={mean1:.4f}, std={std1:.4f}\")\n        print(f\"{second_analysis}: mean={mean2:.4f}, std={std2:.4f}\")\n        print(f\"Difference (second - first): {diff:.4f}\")\n\n        print(f\"\\nMethod: {method}\")\n\n        if method == \"nadeau-bengio\":\n            print(f\"T-statistic: {row.get('t_stat', float('nan')):.4f}\")\n            print(f\"P-value:     {row['p_value']:.6f}\")\n\n        elif method == \"permutation\":\n            print(f\"P-value:     {row['p_value']:.6f}\")\n            print(f\"Valid perms: {row.get('n_valid_perms', 'N/A')}\")\n\n        print(\"-\" * 80)\n\n    print(\"\\n\")\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.run","title":"<code>run(name)</code>","text":"<p>Run a PHOTONAI analysis that has already been added to the project.</p> <p>This will: - load the hyperpipe constructor from the analysis folder, - load the stored data <code>X.npy</code> and <code>y.npy</code>, - fit the hyperpipe, and - write PHOTONAI results to the analysis folder.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the analysis (subfolder of <code>project_folder</code>).</p> required <p>Returns:</p> Type Description <code>Hyperpipe</code> <p>The fitted PHOTONAI hyperpipe instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the analysis folder does not exist in the project folder.</p> Source code in <code>photonai_projects/project.py</code> <pre><code>def run(self, name: str):\n    \"\"\"\n    Run a PHOTONAI analysis that has already been added to the project.\n\n    This will:\n    - load the hyperpipe constructor from the analysis folder,\n    - load the stored data `X.npy` and `y.npy`,\n    - fit the hyperpipe, and\n    - write PHOTONAI results to the analysis folder.\n\n    Parameters\n    ----------\n    name : str\n        Name of the analysis (subfolder of `project_folder`).\n\n    Returns\n    -------\n    Hyperpipe\n        The fitted PHOTONAI hyperpipe instance.\n\n    Raises\n    ------\n    ValueError\n        If the analysis folder does not exist in the project folder.\n    \"\"\"\n    # check that analysis folder exists\n    if name not in os.listdir(self.project_folder):\n        raise ValueError(\n            f\"Analysis {name} not found in project folder {self.project_folder}\"\n        )\n\n    analysis_folder = os.path.join(self.project_folder, name)\n    data_folder = os.path.join(analysis_folder, \"data\")\n\n    pipe = self._load_hyperpipe(analysis_folder, name)\n    pipe.output_settings.set_project_folder(analysis_folder)\n    pipe.output_settings.set_log_file()\n    pipe.name = name\n    pipe.project_folder = analysis_folder\n\n    # load data\n    X = np.load(os.path.join(data_folder, \"X.npy\"))\n    y = np.load(os.path.join(data_folder, \"y.npy\"))\n\n    pipe.fit(X, y)\n\n    # if you want to use feature_importances later, you can hook it here\n    # if self.feature_importances:\n    #     ...\n\n    return pipe\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.run_permutation_test","title":"<code>run_permutation_test(name, n_perms=1000, random_state=15, overwrite=False)</code>","text":"<p>Run a local permutation test for a given analysis.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the analysis.</p> required <code>n_perms</code> <code>int</code> <p>Total number of permutation runs, by default 1000.</p> <code>1000</code> <code>random_state</code> <code>int</code> <p>Base random state for generating permutations, by default 15.</p> <code>15</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrite existing permutation results. If False, skip permutations that already have results, by default False.</p> <code>False</code> Source code in <code>photonai_projects/project.py</code> <pre><code>def run_permutation_test(\n    self,\n    name: str,\n    n_perms: int = 1000,\n    random_state: int = 15,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Run a local permutation test for a given analysis.\n\n    Parameters\n    ----------\n    name : str\n        Name of the analysis.\n    n_perms : int, optional\n        Total number of permutation runs, by default 1000.\n    random_state : int, optional\n        Base random state for generating permutations, by default 15.\n    overwrite : bool, optional\n        If True, overwrite existing permutation results. If False,\n        skip permutations that already have results, by default False.\n    \"\"\"\n    perm_runs = range(n_perms)\n    self._run_permutation_test(\n        name=name,\n        random_state=random_state,\n        n_perms=n_perms,\n        overwrite=overwrite,\n        perm_runs=perm_runs,\n    )\n</code></pre>"},{"location":"api/#photonai_projects.project.PhotonaiProject.run_permutation_test_slurm","title":"<code>run_permutation_test_slurm(name, n_perms=1000, random_state=15, overwrite=False, slurm_job_id=None, n_perms_per_job=None)</code>","text":"<p>Run a subset of permutation tests for use in a SLURM array job.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the analysis.</p> required <code>n_perms</code> <code>int</code> <p>Total number of permutation runs, by default 1000.</p> <code>1000</code> <code>random_state</code> <code>int</code> <p>Base random state for permutation generation, by default 15.</p> <code>15</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing permutation results, by default False.</p> <code>False</code> <code>slurm_job_id</code> <code>int or None</code> <p>Index of the SLURM array job (starting at 1).</p> <code>None</code> <code>n_perms_per_job</code> <code>int or None</code> <p>Number of permutations to run in this job.</p> <code>None</code> Source code in <code>photonai_projects/project.py</code> <pre><code>def run_permutation_test_slurm(\n    self,\n    name: str,\n    n_perms: int = 1000,\n    random_state: int = 15,\n    overwrite: bool = False,\n    slurm_job_id: int | None = None,\n    n_perms_per_job: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Run a subset of permutation tests for use in a SLURM array job.\n\n    Parameters\n    ----------\n    name : str\n        Name of the analysis.\n    n_perms : int, optional\n        Total number of permutation runs, by default 1000.\n    random_state : int, optional\n        Base random state for permutation generation, by default 15.\n    overwrite : bool, optional\n        Whether to overwrite existing permutation results, by default False.\n    slurm_job_id : int or None, optional\n        Index of the SLURM array job (starting at 1).\n    n_perms_per_job : int or None, optional\n        Number of permutations to run in this job.\n    \"\"\"\n    perms_to_do = np.arange(\n        (slurm_job_id - 1) * n_perms_per_job,\n        (slurm_job_id - 1) * n_perms_per_job + n_perms_per_job,\n    )\n    self._run_permutation_test(\n        name=name,\n        random_state=random_state,\n        n_perms=n_perms,\n        overwrite=overwrite,\n        perm_runs=perms_to_do,\n    )\n</code></pre>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install the package (and PHOTONAI) into your environment:</p> <pre><code>pip install photonai photonai-projects \n</code></pre>"},{"location":"getting-started/#basic-concepts","title":"Basic concepts","text":"<p>A PhotonaiProject manages multiple PHOTONAI analyses in a single project folder. Each analysis has its own subfolder containing:</p> <ul> <li> <p>a hyperpipe constructor script (hyperpipe_constructor.py)</p> </li> <li> <p>a metadata file specifying the function name that creates the hyperpipe (hyperpipe_meta.json)</p> </li> <li> <p>a data/ folder with X.npy and y.npy</p> </li> <li> <p>(optionally) a permutations/ folder for results of the permutation test</p> </li> </ul> <p>The typical workflow is:</p> <ol> <li>Create a project with PhotonaiProject. </li> <li>Add analyses (data + hyperpipe constructor). </li> <li>Run analyses to train and evaluate the models. </li> <li>Run permutation tests to obtain null distributions. </li> <li>Compare analyses statistically.</li> </ol>"},{"location":"getting-started/#minimal-example","title":"Minimal example","text":"<p>Below is a complete example using the breast cancer dataset from scikit-learn. We create three analyses using different feature sets, run them, run permutation tests, and then compare them statistically.</p> <pre><code>from photonai_projects.project import PhotonaiProject\nfrom sklearn.datasets import load_breast_cancer\n\n# Load example data\nX, y = load_breast_cancer(return_X_y=True)\n\n# Split features into different sets\nX_1 = X[:, :3]\nX_2 = X[:, 3:6]\n\n# Create a project\nproject = PhotonaiProject(project_folder=\"example_project\")\n</code></pre> <p>You can now add multiple analyses that e.g. use different sets of features. You need to pass the data (X, y) as if you were to call .fit() on a hyperpipe. The data arrays are then saved to disk which makes it easy to access them when running an analysis or performing the permutation test. This also makes it easy to simply rsync everything to an HPC cluster and run analyses there. Instead of creating a hyperpipe during runtime, you pass the location of a Python script that contains a function (hyperpipe constructor) that creates the PHOTONAI Hyperpipe you want to use in this project. <pre><code># ---------------------------------------------------------------------\n# 1) Register analyses\n# ---------------------------------------------------------------------\nfor name, current_X in [(\"all_features\", X), (\"first_feature_set\", X_1), (\"second_feature_set\", X_2)]:\n    project.add(\n        name=name,\n        X=current_X,\n        y=y,\n        hyperpipe_script=\"path/to/hyperpipe_constructor.py\",\n        name_hyperpipe_constructor=\"create_hyperpipe\",\n    )\n\nproject.list_analyses()\n</code></pre> Your PHOTONAI Hyperpipe constructor might look something like this. <pre><code>from photonai import Hyperpipe, PipelineElement\nfrom sklearn.model_selection import KFold\n\n\ndef create_hyperpipe():\n    my_pipe = Hyperpipe('',\n                        optimizer='grid_search',\n                        metrics=['accuracy', 'precision', 'recall'],\n                        best_config_metric='accuracy',\n                        outer_cv=KFold(n_splits=10),\n                        inner_cv=KFold(n_splits=2),\n                        verbosity=1,\n                        project_folder='')\n\n    # Add transformer elements\n    my_pipe += PipelineElement(\"StandardScaler\", hyperparameters={},\n                               test_disabled=True, with_mean=True, with_std=True)\n\n    my_pipe += PipelineElement(\"PCA\", test_disabled=False)\n\n    # Add estimator\n    my_pipe += PipelineElement(\"SVC\", hyperparameters={'kernel': ['linear', 'rbf']},\n                               gamma='scale', max_iter=10000)\n\n    return my_pipe\n</code></pre></p> <p>Now, when you want to run an analysis, you can refer to it by its name and simply call .run(). You can perform a permutation test in the same way. <pre><code># ---------------------------------------------------------------------\n# 2) Run analyses\n# ---------------------------------------------------------------------\nproject.run(name=\"all_features\")\nproject.run(name=\"first_feature_set\")\nproject.run(name=\"second_feature_set\")\n\n# ---------------------------------------------------------------------\n# 3) Run permutation tests (local example)\n# ---------------------------------------------------------------------\n# Use a small number of permutations for testing; increase for real studies.\nproject.run_permutation_test(name=\"all_features\", n_perms=1000)\nproject.run_permutation_test(name=\"first_feature_set\", n_perms=1000)\nproject.run_permutation_test(name=\"second_feature_set\", n_perms=1000)\n</code></pre></p> <p>If you want to compare two PHOTONAI analyses, you can use the .compare_analyses() method which either uses the Nadeau-Bengio corrected t-test or relies on the permutations that have been computed in the individual  significance test of each analysis. <pre><code># ---------------------------------------------------------------------\n# 4) Statistical comparison of analyses\n# ---------------------------------------------------------------------\n# For the Nadeau\u2013Bengio test you must provide n_train and n_test as used\n# during cross-validation. Here we give a simple example.\n# Compare two analyses (Nadeau\u2013Bengio corrected t-test)\nproject.compare_analyses(\n    first_analysis=\"first_feature_set\",\n    second_analysis=\"second_feature_set\",\n    method=\"nadeau-bengio\",\n    n_train=9,\n    n_test=1,\n)\n\n# Compare two analyses (permutation-based)\nproject.compare_analyses(\n    first_analysis=\"all_features\",\n    second_analysis=\"second_feature_set\",\n    method=\"permutation\",\n    n_perms=1000,\n)\n\n# Compare all pairs at once (optional)\nmulti_results = project.compare_multiple_analyses(\n    analyses=[\"all_features\", \"first_feature_set\", \"second_feature_set\"],\n    method=\"permutation\",\n    n_perms=1000,\n)\nprint(multi_results.head())\n</code></pre></p>"},{"location":"getting-started/#running-permutation-tests-on-a-slurm-cluster","title":"Running permutation tests on a SLURM cluster","text":"<p>For large numbers of permutations, you can distribute them across a SLURM array:</p> <pre><code>project.prepare_slurm_permutation_test(\n    name=\"second_feature_set\",\n    n_perms=1000,\n    conda_env=\"my_photonai_env\",\n    memory_per_cpu=2,\n    n_jobs=20,\n    run_time=\"0-02:00:00\",\n    random_state=1,\n)\n</code></pre> <p>This creates a slurm_job.cmd script in the analysis folder which you can submit with:</p> <pre><code>cd example_project/second_feature_set\nsbatch slurm_job.cmd\n</code></pre> <p>Each array job will call the Typer CLI entry point run_perm_job and execute a subset of permutation runs.</p>"},{"location":"getting-started/#next-steps","title":"Next steps","text":"<p>See the Usage page for more details on:</p> <ul> <li>how to design your hyperpipe constructor, </li> <li>how metrics and scorers are handled, </li> <li>how to interpret the comparison reports.</li> </ul> <p>See the API Reference for the full documentation of PhotonaiProject.</p>"},{"location":"report/","title":"Generate an HTML report for your project","text":""},{"location":"report/#example-code","title":"Example code","text":"<p>You can generate an HTML report that summarizes all of your analyses for a project. This makes it easy to get a quick overview of all results but you can also access more detailed results for every analysis.</p> <pre><code># load an existing project\nproject = PhotonaiProject(name='breast_cancer', directory='./')\n\n# then call the collect_results and write_report() function\nproject.collect_results()\nproject.write_report()\n</code></pre>"},{"location":"report/#showcase","title":"Showcase","text":"<p>Here, you can see how such an HTML report looks like. Simply click through the individual tabs and look at the results.</p>"},{"location":"examples/breast-cancer/","title":"Breast cancer","text":"<p>Below is a complete example using the breast cancer dataset from scikit-learn. We create three analyses using different feature sets, run them, run permutation tests, and then compare them statistically.</p> <pre><code>from photonai_projects.project import PhotonaiProject\nfrom sklearn.datasets import load_breast_cancer\n\n# Load example data\nX, y = load_breast_cancer(return_X_y=True)\n\n# Split features into different sets\nX_1 = X[:, :3]\nX_2 = X[:, 3:6]\n\n# Create a project\nproject = PhotonaiProject(project_folder=\"example_project\")\n\n# ---------------------------------------------------------------------\n# 1) Register analyses\n# ---------------------------------------------------------------------\nfor name, current_X in [\n    (\"all_features\", X),\n    (\"first_feature_set\", X_1),\n    (\"second_feature_set\", X_2),\n]:\n    project.add(\n        name=name,\n        X=current_X,\n        y=y,\n        hyperpipe_script=\"path/to/hyperpipe_constructor.py\",\n        name_hyperpipe_constructor=\"create_hyperpipe\",\n    )\n\nproject.list_analyses()\n\n# ---------------------------------------------------------------------\n# 2) Run analyses\n# ---------------------------------------------------------------------\nfor name in [\"all_features\", \"first_feature_set\", \"second_feature_set\"]:\n    project.run(name=name)\n\n# ---------------------------------------------------------------------\n# 3) Run permutation tests (local example)\n# ---------------------------------------------------------------------\n# Use a small number of permutations for testing; increase for real studies.\nfor name in [\"all_features\", \"first_feature_set\", \"second_feature_set\"]:\n    project.run_permutation_test(name=name, n_perms=1000, overwrite=True)\n\n# ---------------------------------------------------------------------\n# 4) Statistical comparison of analyses\n# ---------------------------------------------------------------------\n# Compare two analyses (Nadeau\u2013Bengio corrected t-test)\nproject.compare_analyses(\n    first_analysis=\"first_feature_set\",\n    second_analysis=\"second_feature_set\",\n    method=\"nadeau-bengio\",\n    n_train=9,\n    n_test=1,\n)\n\n# Compare two analyses (permutation-based)\nproject.compare_analyses(\n    first_analysis=\"all_features\",\n    second_analysis=\"second_feature_set\",\n    method=\"permutation\"\n)\n\n# Compare all pairs at once (optional)\nmulti_results = project.compare_multiple_analyses(\n    analyses=[\"all_features\", \"first_feature_set\", \"second_feature_set\"],\n    method=\"permutation\"\n)\n</code></pre>"}]}